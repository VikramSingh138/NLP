{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be46b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 59936 tokens, 1000 sentences, vocab size = 10231\n",
      "Saved probabilities of 1000 sentences to sentence_probabilities.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import csv\n",
    "\n",
    "sentences = []\n",
    "tokens_all = []\n",
    "\n",
    "with open(\"tokenized_sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line_tokens = line.strip().split()\n",
    "        if line_tokens:\n",
    "            sentences.append(line_tokens)\n",
    "            tokens_all.extend(line_tokens)\n",
    "\n",
    "random.seed(42)\n",
    "sentences_1000 = random.sample(sentences, min(1000, len(sentences)))\n",
    "\n",
    "vocab = set(tokens_all)\n",
    "V = len(vocab)\n",
    "print(f\"Loaded {len(tokens_all)} tokens, {len(sentences_1000)} sentences, vocab size = {V}\")\n",
    "\n",
    "def build_ngram(tokens, n):\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "def h_counts(tokens, n):\n",
    "    ngram_counts = build_ngram(tokens, n)\n",
    "    if n > 1:\n",
    "        hcounts = build_ngram(tokens, n-1)\n",
    "    else:\n",
    "        hcounts = {(): len(tokens)}\n",
    "    return ngram_counts, hcounts\n",
    "\n",
    "def smoothingfunc(ngram_counts, hcounts, vocabsize, smoothing=\"add1\", k=0.5):\n",
    "    probs = {}\n",
    "    if smoothing == \"tokentype\":\n",
    "        uniquefolls = defaultdict(set)\n",
    "        for ngram in ngram_counts:\n",
    "            h, w = ngram[:-1], ngram[-1]\n",
    "            uniquefolls[h].add(w)\n",
    "\n",
    "    for ngram, count in ngram_counts.items():\n",
    "        h = ngram[:-1]\n",
    "        hcount = hcounts.get(h, 0)\n",
    "        if smoothing == \"add1\":\n",
    "            probs[ngram] = (count + 1) / (hcount + vocabsize)\n",
    "        elif smoothing == \"addk\":\n",
    "            probs[ngram] = (count + k) / (hcount + k * vocabsize)\n",
    "        elif smoothing == \"tokentype\":\n",
    "            v_h = len(uniquefolls[h]) if h in uniquefolls else 1\n",
    "            probs[ngram] = (count + v_h) / (hcount + v_h * vocabsize)\n",
    "        else:\n",
    "            probs[ngram] = count / hcount if hcount > 0 else 0.0\n",
    "    return probs\n",
    "\n",
    "def sentence_prob(sentence_tokens, n, probs, vocabsize, smoothing=\"add1\", k=0.5):\n",
    "    prob = 1.0\n",
    "    sentence_tokens = [\"<s>\"]*(n-1) + sentence_tokens + [\"</s>\"]\n",
    "    for i in range(n-1, len(sentence_tokens)):\n",
    "        ngram = tuple(sentence_tokens[i-(n-1):i+1])\n",
    "        prob *= probs.get(ngram, 1 / (vocabsize if smoothing==\"add1\" else k*vocabsize))\n",
    "    return prob\n",
    "\n",
    "ngram_probs = {}\n",
    "for n in [1,2,3,4]:\n",
    "    ngram_counts, hcounts = h_counts(tokens_all, n)\n",
    "    \n",
    "    add1_probs = smoothingfunc(ngram_counts, hcounts, V, \"add1\")\n",
    "    addk_probs = smoothingfunc(ngram_counts, hcounts, V, \"addk\", k=0.5)\n",
    "    tokentype_probs = smoothingfunc(ngram_counts, hcounts, V, \"tokentype\")\n",
    "    \n",
    "    ngram_probs[n] = {\n",
    "        \"add1\": add1_probs,\n",
    "        \"addk\": addk_probs,\n",
    "        \"tokentype\": tokentype_probs\n",
    "    }\n",
    "\n",
    "output_file = \"sentence_probabilities.csv\"\n",
    "header = [\"sentence_id\"] + [f\"{n}-gram_{s}\" for n in [1,2,3,4] for s in [\"add1\",\"addk\",\"tokentype\"]]\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for idx, sent_tokens in enumerate(sentences_1000):\n",
    "        row = [idx+1]\n",
    "        for n in [1,2,3,4]:\n",
    "            for smoothing in [\"add1\",\"addk\",\"tokentype\"]:\n",
    "                prob = sentence_prob(sent_tokens, n, ngram_probs[n][smoothing], V, smoothing=smoothing)\n",
    "                row.append(prob)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Saved probabilities of 1000 sentences to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
