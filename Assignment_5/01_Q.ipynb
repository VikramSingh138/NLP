{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae9f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7e056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram(tokens, n):\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i + n])\n",
    "        ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "def h_counts(tokens, n):\n",
    "    ngram_counts = build_ngram(tokens, n)\n",
    "    if n > 1:\n",
    "        hcounts = build_ngram(tokens, n - 1)\n",
    "    else:\n",
    "        hcounts = {(): len(tokens)}\n",
    "    return ngram_counts, hcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691f49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothingfunc(ngram_counts, hcounts, vocabsize, smoothing=\"add1\", k=0.5):\n",
    "    probs = {}\n",
    "    if smoothing == \"tokentype\":\n",
    "        uniquefolls = defaultdict(set)\n",
    "        for ngram in ngram_counts:\n",
    "            h, w = ngram[:-1], ngram[-1]\n",
    "            uniquefolls[h].add(w)\n",
    "\n",
    "    for ngram, count in ngram_counts.items():\n",
    "        h = ngram[:-1]\n",
    "        hcount = hcounts.get(h, 0)\n",
    "        if smoothing == \"add1\":\n",
    "            probs[ngram] = (count + 1) / (hcount + vocabsize)\n",
    "        elif smoothing == \"addk\":\n",
    "            probs[ngram] = (count + k) / (hcount + k * vocabsize)\n",
    "        elif smoothing == \"tokentype\":\n",
    "            v_h = len(uniquefolls[h]) if h in uniquefolls else 1\n",
    "            probs[ngram] = (count + v_h) / (hcount + v_h * vocabsize)\n",
    "        else:\n",
    "            probs[ngram] = count / hcount if hcount > 0 else 0.0\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6282da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_turing_probs(ngram_counts, vocabsize, n):\n",
    "    counts = list(ngram_counts.values())\n",
    "    N = len(ngram_counts)\n",
    "    N1 = sum(1 for c in counts if c == 1)\n",
    "    \n",
    "    # Probability for unseen n-grams\n",
    "    if n == 1:\n",
    "        unseen_vocab = vocabsize - N\n",
    "        unseen_prob = (N1 / N) / unseen_vocab if unseen_vocab > 0 else 0.0\n",
    "    else:\n",
    "        unseen_vocab = vocabsize**n - N\n",
    "        unseen_prob = (N1 / N) / unseen_vocab if unseen_vocab > 0 else 0.0\n",
    "    \n",
    "    # Probabilities for seen n-grams (MLE)\n",
    "    total_count = sum(counts)\n",
    "    probs = {ng: c / total_count for ng, c in ngram_counts.items()}\n",
    "    \n",
    "    return probs, unseen_prob\n",
    "\n",
    "\n",
    "def sentence_prob_good_turing(sentence_tokens, n, probs, unseen_prob):\n",
    "    prob = 1.0\n",
    "    sentence_tokens = [\"<s>\"]*(n-1) + sentence_tokens + [\"</s>\"]\n",
    "    for i in range(n-1, len(sentence_tokens)):\n",
    "        ngram = tuple(sentence_tokens[i-(n-1):i+1])\n",
    "        prob *= probs.get(ngram, unseen_prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44828946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleted_interpolation(sentence_tokens, ngram_probs_list, lambdas , ngram_sizes=[1,2,3,4]):\n",
    "    prob = 1.0\n",
    "    max_n = max(ngram_sizes)\n",
    "    tokens = [\"<s>\"]*(max_n-1) + sentence_tokens + [\"</s>\"]\n",
    "\n",
    "    for i in range(max_n-1, len(tokens)):\n",
    "        p = 0.0\n",
    "        for idx, n in enumerate(ngram_sizes):\n",
    "            ng = tuple(tokens[i-(n-1):i+1])\n",
    "            p += lambdas[idx] * ngram_probs_list[idx].get(ng, 1e-8)  # tiny fallback\n",
    "        prob *= p\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0608746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_frequencies_table(ngram_counts):\n",
    "    counts = list(ngram_counts.values())\n",
    "    C_freq = defaultdict(int)\n",
    "    \n",
    "    for c in counts:\n",
    "        C_freq[c] += 1\n",
    "    \n",
    "    table = []\n",
    "    for c in sorted(C_freq.keys()):\n",
    "        Nc = C_freq[c]\n",
    "        C_star = c  # placeholder for advanced Good-Turing C*\n",
    "        table.append((c, Nc, C_star))\n",
    "    \n",
    "    df = pd.DataFrame(table, columns=[\"C (MLE)\", \"Nc\", \"C*\"])\n",
    "    return df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a72ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 59936 tokens, 1000 sentences, vocab size = 10231\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "tokens_all = []\n",
    "\n",
    "with open(\"tokenized_sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line_tokens = line.strip().split()\n",
    "        if line_tokens:\n",
    "            sentences.append(line_tokens)\n",
    "            tokens_all.extend(line_tokens)\n",
    "\n",
    "vocab = set(tokens_all)\n",
    "V = len(vocab)\n",
    "print(f\"Loaded {len(tokens_all)} tokens, {len(sentences)} sentences, vocab size = {V}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4ff572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1-gram model...\n",
      "1-gram model done. Example probabilities (Add-1):\n",
      "('लोगों',): 0.001981\n",
      "('को',): 0.015221\n",
      "('बिलों',): 0.000029\n",
      "('संबंधी',): 0.000057\n",
      "\n",
      "Building 2-gram model...\n",
      "2-gram model done. Example probabilities (Add-1):\n",
      "('लोगों', 'को'): 0.004533\n",
      "('को', 'बिलों'): 0.000177\n",
      "('बिलों', 'संबंधी'): 0.000195\n",
      "('संबंधी', 'सुविधा'): 0.000195\n",
      "\n",
      "Building 3-gram model...\n",
      "3-gram model done. Example probabilities (Add-1):\n",
      "('लोगों', 'को', 'बिलों'): 0.000195\n",
      "('को', 'बिलों', 'संबंधी'): 0.000195\n",
      "('बिलों', 'संबंधी', 'सुविधा'): 0.000195\n",
      "('संबंधी', 'सुविधा', 'देना'): 0.000195\n",
      "\n",
      "Building 4-gram model...\n",
      "4-gram model done. Example probabilities (Add-1):\n",
      "('लोगों', 'को', 'बिलों', 'संबंधी'): 0.000195\n",
      "('को', 'बिलों', 'संबंधी', 'सुविधा'): 0.000195\n",
      "('बिलों', 'संबंधी', 'सुविधा', 'देना'): 0.000195\n",
      "('संबंधी', 'सुविधा', 'देना', 'ही'): 0.000195\n"
     ]
    }
   ],
   "source": [
    "ngram_probs = {}\n",
    "unseen_probs = {}\n",
    "\n",
    "def good_turing_probs(ngram_counts, vocabsize, n):\n",
    "    counts = list(ngram_counts.values())\n",
    "    N = len(ngram_counts)\n",
    "    N1 = sum(1 for c in counts if c == 1)\n",
    "\n",
    "    # Probability for unseen n-grams\n",
    "    if n == 1:\n",
    "        unseen_vocab = max(vocabsize - N, 1)  # avoid zero division\n",
    "        unseen_prob = (N1 / N) / unseen_vocab\n",
    "    else:\n",
    "        unseen_vocab = max(vocabsize**n - N, 1)\n",
    "        unseen_prob = (N1 / N) / unseen_vocab\n",
    "\n",
    "    # Probabilities for seen n-grams (MLE)\n",
    "    total_count = sum(counts)\n",
    "    probs = {ng: c / total_count for ng, c in ngram_counts.items()}\n",
    "\n",
    "    return probs, unseen_prob\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    print(f\"\\nBuilding {n}-gram model...\")\n",
    "    ngram_counts, hcounts = h_counts(tokens_all, n)\n",
    "\n",
    "    # Different smoothing methods\n",
    "    add1_probs = smoothingfunc(ngram_counts, hcounts, V, \"add1\")\n",
    "    addk_probs = smoothingfunc(ngram_counts, hcounts, V, \"addk\", k=0.5)\n",
    "    tokentype_probs = smoothingfunc(ngram_counts, hcounts, V, \"tokentype\")\n",
    "\n",
    "    # Good-Turing\n",
    "    gt_probs, unseen = good_turing_probs(ngram_counts, V, n)\n",
    "    ngram_probs[n] = gt_probs\n",
    "    unseen_probs[n] = unseen\n",
    "\n",
    "    print(f\"{n}-gram model done. Example probabilities (Add-1):\")\n",
    "    for i, (ng, p) in enumerate(add1_probs.items()):\n",
    "        print(f\"{ng}: {p:.6f}\")\n",
    "        if i >= 3:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a043b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 28250, Test tokens: 31686, Vocab size: 6335\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sentences = []\n",
    "tokens_all = []\n",
    "\n",
    "with open(\"tokenized_sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line_tokens = line.strip().split()\n",
    "        if line_tokens:\n",
    "            sentences.append(line_tokens)\n",
    "            tokens_all.extend(line_tokens)\n",
    "\n",
    "# Shuffle sentences for randomness\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Split 50-50\n",
    "split_idx = len(sentences) // 2\n",
    "train_sentences = sentences[:split_idx]\n",
    "test_sentences = sentences[split_idx:]\n",
    "\n",
    "# Flatten tokens for training\n",
    "train_tokens = [tok for sent in train_sentences for tok in sent]\n",
    "test_tokens = [tok for sent in test_sentences for tok in sent]\n",
    "\n",
    "V = len(set(train_tokens))\n",
    "print(f\"Train tokens: {len(train_tokens)}, Test tokens: {len(test_tokens)}, Vocab size: {V}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ac89a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_probs = {}\n",
    "unseen_probs = {}\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    ngram_counts, hcounts = h_counts(train_tokens, n)\n",
    "    \n",
    "    # Smoothing\n",
    "    add1_probs = smoothingfunc(ngram_counts, hcounts, V, \"add1\")\n",
    "    addk_probs = smoothingfunc(ngram_counts, hcounts, V, \"addk\", k=0.5)\n",
    "    tokentype_probs = smoothingfunc(ngram_counts, hcounts, V, \"tokentype\")\n",
    "    \n",
    "    # Good-Turing\n",
    "    gt_probs, unseen = good_turing_probs(ngram_counts, V, n)\n",
    "    ngram_probs[n] = gt_probs\n",
    "    unseen_probs[n] = unseen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c4ab224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: 4.486316e-142\n",
      "Sentence 2: 0.000000e+00\n",
      "Sentence 3: 3.345402e-107\n",
      "Sentence 4: 3.744029e-320\n",
      "Sentence 5: 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "def sentence_prob(sentence_tokens, n, probs, unseen_prob):\n",
    "    prob = 1.0\n",
    "    sentence_tokens = [\"<s>\"]*(n-1) + sentence_tokens + [\"</s>\"]\n",
    "    for i in range(n-1, len(sentence_tokens)):\n",
    "        ngram = tuple(sentence_tokens[i-(n-1):i+1])\n",
    "        prob *= probs.get(ngram, unseen_prob)\n",
    "    return prob\n",
    "\n",
    "# Example: probability for test set sentences using 4-gram Good-Turing\n",
    "sentence_probs = []\n",
    "for sent in test_sentences:\n",
    "    p = sentence_prob(sent, 4, ngram_probs[4], unseen_probs[4])\n",
    "    sentence_probs.append(p)\n",
    "\n",
    "# Print first 5\n",
    "for i, p in enumerate(sentence_probs[:5]):\n",
    "    print(f\"Sentence {i+1}: {p:.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d44ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    C (MLE)     Nc   C*\n",
      "0         1  18216    1\n",
      "1         2   1704    2\n",
      "2         3    517    3\n",
      "3         4    237    4\n",
      "4         5    104    5\n",
      "5         6     70    6\n",
      "6         7     46    7\n",
      "7         8     37    8\n",
      "8         9     15    9\n",
      "9        10     22   10\n",
      "10       11     10   11\n",
      "11       12     13   12\n",
      "12       13     10   13\n",
      "13       14     12   14\n",
      "14       15      7   15\n",
      "15       16      6   16\n",
      "16       17      8   17\n",
      "17       18      1   18\n",
      "18       19      4   19\n",
      "19       21      7   21\n",
      "20       22      1   22\n",
      "21       23      1   23\n",
      "22       24      2   24\n",
      "23       25      2   25\n",
      "24       26      1   26\n",
      "25       27      2   27\n",
      "26       28      1   28\n",
      "27       29      1   29\n",
      "28       32      2   32\n",
      "29       34      1   34\n",
      "30       35      1   35\n",
      "31       36      1   36\n",
      "32       37      1   37\n",
      "33       54      1   54\n",
      "34       56      1   56\n",
      "35       58      1   58\n",
      "36       63      1   63\n",
      "37       68      1   68\n",
      "38      105      1  105\n",
      "39      181      1  181\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def top_frequencies_table(ngram_counts):\n",
    "    counts = list(ngram_counts.values())\n",
    "    C_freq = defaultdict(int)\n",
    "    \n",
    "    for c in counts:\n",
    "        C_freq[c] += 1\n",
    "    \n",
    "    table = []\n",
    "    for c in sorted(C_freq.keys()):\n",
    "        Nc = C_freq[c]\n",
    "        C_star = c  # For simplicity, can be replaced with GT-adjusted C*\n",
    "        table.append((c, Nc, C_star))\n",
    "    \n",
    "    df = pd.DataFrame(table, columns=[\"C (MLE)\", \"Nc\", \"C*\"])\n",
    "    return df.head(100)\n",
    "\n",
    "# Example for bigrams\n",
    "bigram_counts, _ = h_counts(train_tokens, 2)\n",
    "freq_table = top_frequencies_table(bigram_counts)\n",
    "print(freq_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6229eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted interpolated probability: 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "def deleted_interpolation(sentence_tokens, ngram_probs_list, lambdas, ngram_sizes=[1,2,3,4]):\n",
    "    prob = 1.0\n",
    "    max_n = max(ngram_sizes)\n",
    "    tokens = [\"<s>\"]*(max_n-1) + sentence_tokens + [\"</s>\"]\n",
    "    for i in range(max_n-1, len(tokens)):\n",
    "        p = 0.0\n",
    "        for idx, n in enumerate(ngram_sizes):\n",
    "            ng = tuple(tokens[i-(n-1):i+1])\n",
    "            p += lambdas[idx] * ngram_probs_list[idx].get(ng, 1e-8)  # fallback tiny prob\n",
    "        prob *= p\n",
    "    return prob\n",
    "\n",
    "# Optimize lambdas (sum=1) for quadrigrams using train set\n",
    "best_lambdas = [0.25, 0.25, 0.25, 0.25]  # Can run grid search over lambda combinations\n",
    "ngram_probs_list = [ngram_probs[n] for n in [1,2,3,4]]\n",
    "\n",
    "# Example: probability of first test sentence\n",
    "p = deleted_interpolation(test_sentences[0], ngram_probs_list, best_lambdas)\n",
    "print(f\"Deleted interpolated probability: {p:.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1454261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved probabilities for 1000 sentences to sentence_probabilities.json\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "num_sentences = min(1000, len(sentences))\n",
    "sentence_probs = []\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent_tokens = sentences[i]\n",
    "    \n",
    "    # Example using quadrigram Good-Turing (n=4)\n",
    "    prob = sentence_prob_good_turing(sent_tokens, 4, ngram_probs[4], unseen_probs[4])\n",
    "    \n",
    "    sentence_probs.append({\n",
    "        \"sentence_index\": i,\n",
    "        \"sentence\": \" \".join(sent_tokens),\n",
    "        \"probability\": prob\n",
    "    })\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"sentence_probabilities.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_probs, f, indent=2)\n",
    "\n",
    "print(f\"Saved probabilities for {num_sentences} sentences to sentence_probabilities.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
